---
title: "Gaussian Model Trained with Unconstrained HMC"
author: "Sara Taheri and Robert Osazwa Ness"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r, warning=FALSE, message=FALSE, include=FALSE}
library(purrr)
library(rstan)
library(bayesplot)
library(ggplot2)
options(mc.cores = parallel::detectCores())
rstan_options(auto_write = TRUE)
```

```{r}
seed = 234
set.seed(seed)
L = 5
D = 1000
N = 8
```

We have a causal model that consists of four variables $U$, $X$, $M$ and $Y$ that have dimension of is $D \times L$, $D \times N$, $D \times 1$, and $D \times 1$ respectively. $U$ is a hidden confounder. Our causal model is as follows:


$$ U := N_{U} \\
X := U \alpha + N_{X} \\
M := X \beta + N_{M} \\
Y := M \tau + U \eta + N_{Y} = [M \text{  } U] [\tau \text{  } \eta]^T = [M \text{  } U] \gamma  + N_{Y}$$


We assume that,

$$ U \sim N(\mu,\Sigma_{UU}) \\
X \sim N(\mu \alpha , \Sigma_{XX}) \\
M \sim N(\mu \alpha \beta, \Sigma_{YY})
Y \sim N(\mu \alpha \beta + \mu \eta, \Sigma_{YY}) = N([\mu \alpha \text{   } \mu] \gamma, \Sigma_{YY})$$

For now we assume that $\Sigma_{UU}$, $\Sigma_{XX}$, $\Sigma_{MM}$ , and $\Sigma_{YY}$ are identity matrix. The dimensions of $\mu$, $\alpha$, $\beta$, and $\gamma$ are $1 \times L$, $L \times N$, $N \times 1$, and $L+1 \times 1$ respectively.

Let's calculate $P(Y | do(X = x))$:

$$P(Y | do(X = x)) \sim N(x \beta \tau + \mu \eta, \Sigma_{YY|do(x)}) = N([x \beta \text{   } \mu] \text{   } \gamma, \Sigma_{YY|do(x)})$$

For now we assume that $\Sigma_{YY|do(x)}$ is 1. If $\mu = 0$, then $P(Y | do(X = x))$ is fully parametrized by $\beta$ and $\tau$, but if $\mu \neq 0$, then $P(Y | do(X = x))$ is parametrized by $\beta$, $\tau$, $\mu$ and $\eta$ or in other words by $\beta$, $\mu$ and $\gamma$.

# Create data for U,X,Y and parameters

The data for $U$, $X$, $M$, $Y$ and the parameters $\mu$, $\alpha$, $\beta$, and $\gamma$ can be found in the data folder, under the names of ```u_train```, ```x_train```, ```m_train```, ```y_train```, ```mu```, ```alpha```, ```beta```, and ```gamma``` respectively. For some reason, ```x_train``` and ```y_train``` didn't work for me, so I will include the whole data generating process here:

```{r, echo=FALSE}
f_u <- function(){
  mu <- rnorm(L)
  u <- matrix(0, nrow=D, ncol=L)
  for(i in 1:D){
    for(j in 1:L){
      u[i, j] <- rnorm(1, mu[j])
    }
  }
  return(list(u = u, mu = mu))
}
sim <- f_u()
mu <- sim$mu
u_train  <- sim$u

f_x <- function(u){
  alpha <- matrix(0, nrow = L, ncol = N) 
  for(i in 1:L){
    for(j in 1:N){
      alpha[i, j] <- rnorm(1, 0, 10)
    }
  }
  linear_exp = u %*% alpha
  x <- matrix(0, nrow = D, ncol = N)
  for(i in 1:D){
    for(j in 1:N){
      x[i, j] <- rnorm(1, linear_exp[i,j],1)
    }
  }
  return(list(x = x, alpha = alpha))
}
sim_x <- f_x(u_train)
alpha <- sim_x$alpha
x_train  <- sim_x$x

f_m <- function(x){
  beta <- matrix(0, nrow = N, ncol = 1) 
  for(i in 1:N){
    for(j in 1:1){
      beta[i, j] <- rnorm(1, 0, 10)
    }
  }
  linear_exp = x %*% beta
  m <- matrix(0, nrow = D, ncol = 1)
  for(i in 1:D){
    for(j in 1:1){
      m[i, j] <- rnorm(1, linear_exp[i,j],1)
    }
  }
  return(list(m = m, beta = beta))
}
sim_m <- f_m(x_train)
beta <- sim_m$beta
m_train  <- sim_m$m

f_y <- function(m,u){
  gamma <- matrix(0, nrow = (L+1), ncol = 1) 
  for(i in 1:(L+1)){
    for(j in 1:1){
      gamma[i, j] <- rnorm(1, 0, 10)
    }
  }
  m_and_u <- cbind(m,u)
  linear_exp = m_and_u %*% gamma
  y <- matrix(0, nrow = D, ncol = 1)
  for(i in 1:D){
    for(j in 1:1){
      y[i, j] <- rnorm(1, linear_exp[i,j],1)
    }
  }
  return(list(y = y, gamma = gamma))
}
sim_y <- f_y(m_train,u_train)
gamma <- sim_y$gamma
y_train  <- sim_y$y
```

# True distribution

Let's calculate the true mean of $P(Y | do(X = x))$ distribution,

```{r}
# generate x
sigma_x = matrix(0, nrow = N, ncol = N)
diag(sigma_x) <- 1
x = mvtnorm::rmvnorm(n = 1, mean = mu %*% alpha, sigma = sigma_x)
#generate samples for y from P(Y | do(X = x))
mean_y_given_do_x_alex = (t(matrix(c((x %*% beta), mu))) %*% gamma)
y_given_do_x_alex_samples = rnorm(1000,mean_y_given_do_x_alex,1)
```

### Plot distribution of $P(Y | do(X = x))$

```{r}
hist(y_given_do_x_alex_samples, main = "true distribution of P(Y|do(X = x))") #change to ggplot2
```

# Estimating the parameters of model when there is no hidden confounder

## Stan model

The Stan model can be found in vignette under the name of ```model_str```. Let's compile the model:

```{r, message=FALSE, warning=FALSE}
mod <- rstan::stan_model("model_str.stan")
```

## MLE optimizing approach without hidden confounder

We want to test the model using MLE 'optimizing', because it is faster than Bayesian inference:

```{r, message=FALSE, warning=FALSE}
data_list <- list(u=u_train, L=L, D=D, x = x_train, N =N, m = m_train, y = y_train, m_and_u = cbind(m_train,u_train))
```

```{r}
optim_fit <- rstan::optimizing(mod, data=data_list)
```

Let's compare the estimates of parameters with their actual values:

```{r}
optim_fit$par[1:5] #estimated mu
mu #real mu
optim_fit$par[6:45] #estimated alpha
t(alpha) #real alpha
optim_fit$par[46:53] #estimated beta
t(beta) #real beta
optim_fit$par[54:59] #estimated gamma
t(gamma) #real gamma
```

It seems like the parameters are closely estimated except for $\mu$ and $\gamma$. 

## HMC without hidden confounders

Now we will proceed with HMC inference. ```hmc_fit``` is the result of HMC run with 2000 iteration, 1000 warmup, 2 chains, and a fixed seed. 

```{r fig.height=8, fig.width=8}
stan_trace(pars = "mu", knocker::hmc_fit)
```

```{r fig.height=8, fig.width=8}
stan_trace(pars = "alpha", knocker::hmc_fit)
```

```{r fig.height=8, fig.width=8}
stan_trace(pars = "beta", knocker::hmc_fit)
```

```{r fig.height=8, fig.width=8}
stan_trace(pars = "gamma", knocker::hmc_fit)
```

Now let's extract the samples:

```{r}
samples_mu <- extract(knocker::hmc_fit, "mu")
colnames(samples_mu$mu) <- paste0("mu_", 1:L)
colMeans(samples_mu$mu)
mu
```

```{r}
samples_alpha <- extract(knocker::hmc_fit, "alpha")
samples_alpha_matrix <- matrix(c(colMeans(samples_alpha$alpha[,,1]),
                                 colMeans(samples_alpha$alpha[,,2]),
                                 colMeans(samples_alpha$alpha[,,3]),
                                 colMeans(samples_alpha$alpha[,,4]),
                                 colMeans(samples_alpha$alpha[,,5]),
                                 colMeans(samples_alpha$alpha[,,6]),
                                 colMeans(samples_alpha$alpha[,,7]),
                                 colMeans(samples_alpha$alpha[,,8])),
                               byrow = FALSE, nrow = 5, ncol = 8, dimnames = list(paste0("alpha_", 1:L), paste0("alpha_", 1:N)))
samples_alpha_matrix
alpha
```

```{r}
samples_beta <- extract(knocker::hmc_fit, "beta")
colMeans(samples_beta$beta[,,1])
beta
```

```{r}
samples_gamma <- extract(knocker::hmc_fit, "gamma")
colMeans(samples_gamma$gamma[,,1])
gamma
```

It doesn't learn $\mu$ correctly and some of the $\gamma$ parameters.

# Estimating the parameters in presence of hidden confounder U

The Stan model is in vignette under the name of ```model_str_with_latent_confounder.stan```. Let's complie the model:

```{r, message=FALSE, warning=FALSE}
mod_with_hidden_confounder <- rstan::stan_model("model_str_with_latent_confounder.stan")
```

```{r}
data_list_with_hidden_confounder <- list(L=L, D=D, x = x_train, N =N, y = y_train, m = m_train)
```

## MLE optimizing approach with hidden confounder

```{r}
optim_fit_with_hidden_confounder <- optimizing(mod_with_hidden_confounder, data=data_list_with_hidden_confounder)
```


```{r}
optim_fit_with_hidden_confounder$par[1:5] #estimated mu
mu #real mu
optim_fit_with_hidden_confounder$par[5006:5045] #estimated alpha
alpha #real alpha
optim_fit_with_hidden_confounder$par[5046:5053] #estimated beta
beta #real beta
```


```{r}
optim_fit_with_hidden_confounder$par[5054:5059] #estimated gamma
gamma #real gamma
```

It doesn't learn the parameters correctly when u is hidden except for beta and first parameter of gamma which is basically tau. It learns beta and tau perfectly well!

## HMC in presecne of hidden confounder
 
let's use hmc approach. The result of svi algorithm can be found in data folder, under the name of ```hmc_fit_with_hidden_confounder```.

```{r}
#hmc_fit_with_hidden_confounder <- rstan::sampling(mod_with_hidden_confounder, data=data_list_with_hidden_confounder, chains = 2, iter = 2500, warmup = 1500, seed = seed)
#usethis::use_data(hmc_fit_with_hidden_confounder)
```

```{r fig.height=8, fig.width=8}
stan_trace(pars = "mu", knocker::hmc_fit_with_hidden_confounder)
```

```{r fig.height=8, fig.width=8}
stan_trace(pars = "alpha", knocker::hmc_fit_with_hidden_confounder)
```

```{r fig.height=8, fig.width=8}
stan_trace(pars = "beta", knocker::hmc_fit_with_hidden_confounder)
```

```{r fig.height=8, fig.width=8}
stan_trace(pars = "gamma", knocker::hmc_fit_with_hidden_confounder)
```

Now let's extract the samples:

```{r}
samples_mu <- extract(knocker::hmc_fit_with_hidden_confounder, "mu")
colnames(samples_mu$mu) <- paste0("mu_", 1:L)
colMeans(samples_mu$mu)
mu
```

```{r}
samples_alpha <- extract(knocker::hmc_fit_with_hidden_confounder, "alpha")
samples_alpha_matrix <- matrix(c(colMeans(samples_alpha$alpha[,,1]),
                                 colMeans(samples_alpha$alpha[,,2]),
                                 colMeans(samples_alpha$alpha[,,3]),
                                 colMeans(samples_alpha$alpha[,,4]),
                                 colMeans(samples_alpha$alpha[,,5]),
                                 colMeans(samples_alpha$alpha[,,6]),
                                 colMeans(samples_alpha$alpha[,,7]),
                                 colMeans(samples_alpha$alpha[,,8])),
                               byrow = FALSE, nrow = 5, ncol = 8, dimnames = list(paste0("alpha_", 1:L), paste0("alpha_", 1:N)))
samples_alpha_matrix
alpha
```

```{r}
samples_beta <- extract(knocker::hmc_fit_with_hidden_confounder, "beta")
colMeans(samples_beta$beta[,,1])
beta
```


```{r}
samples_gamma <- extract(knocker::hmc_fit_with_hidden_confounder, "gamma")
colMeans(samples_gamma$gamma[,,1])
gamma
```

Beta parameters are estimated correctly! Tau (The first parameter of gamma) is estimated correctly. If we can show that by enforcing parameter independence we can learn gamma correctly then, this is a contribution!

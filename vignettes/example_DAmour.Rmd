---
title: "Gaussian model DAmour example"
author: "Sara Taheri and Robert Osazwa Ness"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r, warning=FALSE, message=FALSE, echo=FALSE}
library(purrr)
library(rstan)
library(bayesplot)
library(ggplot2)
library(mvtnorm)
options(mc.cores = parallel::detectCores())
rstan_options(auto_write = TRUE)
```

```{r}
seed = 234
set.seed(seed)
L = 5 #number of latent variables
D = 1000 #number of data points
N = 8 #number of causes
``` 

We have a causal model that consists of three variables $U$, $X$, and $Y$ that have dimension of is $D \times L$, $D \times N$, and $D \times 1$ respectively. $U$ is a hidden confounder. Our causal model is as follows:


$$ U := N_{U} \\
X := U \alpha + N_{X} \\
Y := X \beta + U \eta + N_{Y} = [X \text{  } U] [\beta \text{  } \eta]^T = [X \text{  } U] \gamma  + N_{Y}$$


We assume that,

$$ U \sim N(\mu,\Sigma_{UU}) \\
X \sim N(\mu \alpha , \Sigma_{XX}) \\
Y \sim N(\mu \alpha \beta + \mu \eta, \Sigma_{YY}) = N([\mu \alpha \text{   } \mu] \gamma, \Sigma_{YY})$$

For now we assume that $\Sigma_{UU}$, $\Sigma_{XX}$, and $\Sigma_{YY}$ are identity matrix. The dimensions of $\mu$, $\alpha$, and $\gamma$ are $1 \times L$, $L \times N$, and $L+N \times 1$ respectively.

Let's calculate $P(Y | do(X = x))$:

$$P(Y | do(X = x)) \sim N(x \beta + \mu \eta, \Sigma_{YY|do(x)}) = N([x \text{   } \mu] \text{   } \gamma, \Sigma_{YY|do(x)})$$

For now we assume that $\Sigma_{YY|do(x)}$ is 1. If $\mu = 0$, then $P(Y | do(X = x))$ is fully parametrized by $\beta$ but if $\mu \neq 0$, then $P(Y | do(X = x))$ is parametrized by $\beta$, $\mu$ and $\eta$ or in other words by $\mu$ and $\gamma$.

# Create data for U,X,Y and parameters

```f_u```, ```f_x```, and ```f_y``` are functions that generate data for u,x, and y and output the parameters associated with each variable which are $\mu$, $\alpha$, and $\gamma$ respectively.

```{r}
f_u <- function(){
  mu <- rnorm(L)
  u <- matrix(0, nrow=D, ncol=L)
  for(i in 1:D){
    for(j in 1:L){
      u[i, j] <- rnorm(1, mu[j])
    }
  }
  return(list(u = u, mu = mu))
}
sim <- f_u()
mu_alex <- sim$mu
u_train_alex  <- sim$u

f_x <- function(u){
  alpha <- matrix(0, nrow = L, ncol = N) 
  for(i in 1:L){
    for(j in 1:N){
      alpha[i, j] <- rnorm(1, 0, 10)
    }
  }
  linear_exp = u %*% alpha
  x <- matrix(0, nrow = D, ncol = N)
  for(i in 1:D){
    for(j in 1:N){
      x[i, j] <- rnorm(1, linear_exp[i,j],1)
    }
  }
  return(list(x = x, alpha = alpha))
}
sim_x <- f_x(u_train_alex)
alpha_alex <- sim_x$alpha
x_train_alex  <- sim_x$x

f_y <- function(x,u){
  gamma <- matrix(0, nrow = (L+N), ncol = 1) 
  for(i in 1:(L+N)){
    for(j in 1:1){
      gamma[i, j] <- rnorm(1, 0, 10)
    }
  }
  x_and_u <- cbind(x,u)
  linear_exp = x_and_u %*% gamma
  y <- matrix(0, nrow = D, ncol = 1)
  for(i in 1:D){
    for(j in 1:1){
      y[i, j] <- rnorm(1, linear_exp[i,j],1)
    }
  }
  return(list(y = y, gamma = gamma))
}
sim_y <- f_y(x_train_alex,u_train_alex)
gamma_alex <- sim_y$gamma
y_train_alex  <- sim_y$y
```

# True distribution

Let's calculate the true mean of $P(Y | do(X = x))$ distribution,

```{r}
# generate x
sigma_x_alex = matrix(0, nrow = N, ncol = N)
diag(sigma_x_alex) <- 1
x = mvtnorm::rmvnorm(n = 1, mean = mu_alex %*% alpha_alex, sigma = sigma_x_alex)

#generate samples for y from P(Y | do(X = x))
mean_y_given_do_x_alex = (t(matrix(c(x, mu_alex))) %*% gamma_alex)
y_given_do_x_alex_samples = rnorm(1000,mean_y_given_do_x_alex,1)
```

### Plot distribution of $P(Y | do(X = x))$

```{r}
hist(y_given_do_x_alex_samples, main = "true distribution of P(Y|do(X = x))") #change to ggplot2
```

# Estimating the parameters of model when there is no hidden confounder

## Stan model
The Stan model is a string and it specifies the data, parameters, and model.

```{r}
model_str <- "
    data {
        int L;
        int D;
        int N;
        matrix[D, L] u;
        matrix[D, N] x;
        matrix[D, 1] y;
        matrix[D, L+N] x_and_u;
    }
    parameters {
       vector[L] mu; 
       matrix[L, N] alpha;
       matrix[L+N, 1] gamma;
    }
    transformed parameters {
       matrix[D, N] x_loc;
       matrix[D, 1] y_loc;
       for (i in 1:D){
           x_loc[i, ] = u[i, ] * alpha;
           y_loc[i, ] = x_and_u[i,] * gamma;
       }
    }
    model {
        target += normal_lpdf(mu | 0, 1);
        for (j in 1:L){
            target += normal_lpdf(alpha[j, ] | 0, 1);
        }
        for (j in 1:(L+N)){
            target += normal_lpdf(gamma[j, ] | 0, 1);
        }
        for (i in 1:D){
             target += normal_lpdf(u[i, ] | mu, 10);      // likelihood
             target += normal_lpdf(x[i, ] | x_loc[i, ], 1);
             target += normal_lpdf(y[i, ] | y_loc[i, ], 1);
        }
    }
"
```

Let's compile the model:

```{r, message=FALSE, warning=FALSE, results=FALSE}
mod <- stan_model(model_code = model_str)
```

## MLE optimizing approach without hidden confounder

We want to test the model using MLE 'optimizing', because it is faster than Bayesian inference:

```{r}
data_list <- list(u=u_train_alex, L=L, D=D, x = x_train_alex, N =N, y = y_train_alex, x_and_u = cbind(x_train_alex,u_train_alex))
```

```{r}
optim_fit <- optimizing(mod, data=data_list)
```

Let's compare the estimates of parameters with their actual values:

```{r}
optim_fit$par[1:5] #estimated mu
mu_alex #real mu
optim_fit$par[6:45] #estimated alpha
alpha_alex #real alpha
optim_fit$par[46:58] #estimated gamma
gamma_alex #real gamma
```

gamma_9,...,gamma_13 parameters are not estmated correctly. These are essentially our eta parameters. Other than that, the rest of parameters are closely estimated. Let's try HMC and SVI. The result of running SVI when we don't have a hidden confounder is in data folder, under the name ```vb_fit_alex_without_hidden_confounder```. The result of running HMC when we don't have a hidden confounder is in data folder, under the name ```hmc_fit_alex_without_hidden_confounder```. It is the result of HMC run with 2000 iteration, 1000 warmup, 2 chains, and a fixed seed. 

## SVI without hidden confounder

```{r, echo=FALSE, results = FALSE}
#vb_fit_alex_without_hidden_confounder <- rstan::vb(mod, data=data_list, seed = seed)
#usethis::use_data(vb_fit_alex_without_hidden_confounder)
```

```{r}
summary(knocker::vb_fit_alex_without_hidden_confounder)$summary[,"mean"][1:5]
mu_alex #real mu
```
```{r}
summary(knocker::vb_fit_alex_without_hidden_confounder)$summary[,"mean"][6:45]
alpha_alex #real alpha
```

```{r}
summary(knocker::vb_fit_alex_without_hidden_confounder)$summary[,"mean"][46:58]
gamma_alex #real gamma
```

Again, gamma_9,...,gamma_13 parameters are not estmated correctly. These are essentially our eta parameters. Other than that, the rest of parameters are closely estimated.

## HMC without hidden confounders

```{r}
#I added max_treedepth if I want to run this algorithm once again, because I recieved this warning: "There were 2000 transitions after warmup that exceeded the maximum treedepth. Increase max_treedepth above 10."

#hmc_fit_alex_without_hidden_confounder <- rstan::sampling(mod, data=data_list,chains = 2, iter = 2000, warmup = 1000, seed = seed, control = list(max_treedepth = 15)) 
#usethis::use_data(hmc_fit_alex_without_hidden_confounder)
```

```{r fig.height=8, fig.width=8}
stan_trace(pars = "mu", knocker::hmc_fit_alex_without_hidden_confounder)
```

```{r fig.height=8, fig.width=8}
stan_trace(pars = "alpha", knocker::hmc_fit_alex_without_hidden_confounder)
```

```{r fig.height=8, fig.width=8}
stan_trace(pars = "gamma", knocker::hmc_fit_alex_without_hidden_confounder)
```

Now let's extract the samples:

```{r}
samples_mu <- extract(knocker::hmc_fit_alex_without_hidden_confounder, "mu")
colnames(samples_mu$mu) <- paste0("mu_", 1:L)
colMeans(samples_mu$mu)
mu_alex
```

```{r}
samples_alpha <- extract(knocker::hmc_fit_alex_without_hidden_confounder, "alpha")
dim(samples_alpha$alpha)
samples_alpha_matrix <- matrix(c(colMeans(samples_alpha$alpha[,,1]),
                                 colMeans(samples_alpha$alpha[,,2]),
                                 colMeans(samples_alpha$alpha[,,3]),
                                 colMeans(samples_alpha$alpha[,,4]),
                                 colMeans(samples_alpha$alpha[,,5]),
                                 colMeans(samples_alpha$alpha[,,6]),
                                 colMeans(samples_alpha$alpha[,,7]),
                                 colMeans(samples_alpha$alpha[,,8])),
                               byrow = FALSE, nrow = 5, ncol = 8, dimnames = list(paste0("alpha_", 1:L), paste0("alpha_", 1:N)))
samples_alpha_matrix
alpha_alex
```

```{r}
samples_gamma <- extract(knocker::hmc_fit_alex_without_hidden_confounder, "gamma")
colMeans(samples_gamma$gamma[,,1])
gamma_alex
```

It learns all the parameters correctly except for gamma_9,...,gamma_13, which are essentially the eta parameter.

# Estimating the parameters in presence of hidden confounder U

```{r}
model_str_alex_with_hidden_confounder <- "
    data {
        int L;
        int D;
        int N;
        matrix[D, N] x;
        matrix[D, 1] y;
    }
    parameters {
       vector[L] mu;
       matrix[D, L] u;
       matrix[L, N] alpha;
       matrix[L+N, 1] gamma;
    }
    transformed parameters {
       matrix[D, N] x_loc;
       matrix[D, 1] y_loc;
       matrix[D, L+N] x_and_u;
       x_and_u = append_col(x,u);
       for (i in 1:D){
           x_loc[i, ] = u[i, ] * alpha;
           y_loc[i, ] = x_and_u[i, ] * gamma;
       }
    }
    model {
        target += normal_lpdf(mu | 0, 1);
        for (j in 1:L){
            target += normal_lpdf(alpha[j, ] | 0, 1);
        }
        for (j in 1:(L+N)){
            target += normal_lpdf(gamma[j, ] | 0, 1);
        }
        for (i in 1:D){
             target += normal_lpdf(u[i, ] | mu, 10);      // likelihood
             target += normal_lpdf(x[i, ] | x_loc[i, ], 1);
             target += normal_lpdf(y[i, ] | y_loc[i, ], 1);
        }
    }
"
```

Let's compile the model:

```{r, message=FALSE, warning=FALSE, results=FALSE}
mod_alex_with_hidden_confounder <- stan_model(model_code = model_str_alex_with_hidden_confounder)
```

```{r}
data_list_alex_with_hidden_confounder <- list(L=L, D=D, x = x_train_alex, N =N, y = y_train_alex)
```

## MLE optimizing approach with hidden confounder

```{r}
optim_fit_with_hidden_confounder <- optimizing(mod_alex_with_hidden_confounder, data=data_list_alex_with_hidden_confounder)
```


```{r}
optim_fit_with_hidden_confounder$par[1:5] #estimated mu
mu_alex #real mu
#optim_fit_with_hidden_confounder$par[6:(D*L)] #estimated u
#u_train_alex #real u
optim_fit_with_hidden_confounder$par[5006:5045] #estimated alpha
alpha_alex #real alpha
optim_fit_with_hidden_confounder$par[5046:5058] #estimated gamma
gamma_alex #real gamma
```

It doesn't learn the parameters correctly when u is hidden except for gamma1,...,gamma8.

## SVI in presecne of hidden confounder
 
let's use svi approach:

```{r}
#vb_fit_alex_with_hidden_confounder <- rstan::vb(mod_alex_with_hidden_confounder, data=data_list_alex_with_hidden_confounder, seed = seed)
#usethis::use_data(vb_fit_alex_with_hidden_confounder)
```


```{r}
summary(knocker::vb_fit_alex_with_hidden_confounder)$summary[,"mean"][1:5]
mu_alex #real mu
```
```{r}
summary(knocker::vb_fit_alex_with_hidden_confounder)$summary[,"mean"][5006:5045]
alpha_alex #real alpha
```

```{r}
summary(knocker::vb_fit_alex_with_hidden_confounder)$summary[,"mean"][5046:5058]
gamma_alex #real gamma
```

It doesn't learn the parameters correctly when u is hidden.

## HMC in presecne of hidden confounder

```{r}
#hmc_fit_alex_with_hidden_confounder <- rstan::sampling(mod_alex_with_hidden_confounder, data=data_list_alex_with_hidden_confounder, chains = 2, iter = 2500, warmup = 1500, seed = seed,control = list(max_treedepth = 15)) #I added max_treedepth = 15, if I want to run this once again
#usethis::use_data(hmc_fit_alex_with_hidden_confounder)
```

```{r fig.height=8, fig.width=8}
stan_trace(pars = "mu", knocker::hmc_fit_alex_with_hidden_confounder)
```

```{r fig.height=8, fig.width=8}
stan_trace(pars = "alpha", knocker::hmc_fit_alex_with_hidden_confounder)
```

```{r fig.height=8, fig.width=8}
stan_trace(pars = "gamma", knocker::hmc_fit_alex_with_hidden_confounder)
```

Now let's extract the samples:

```{r}
samples_mu <- extract(knocker::hmc_fit_alex_with_hidden_confounder, "mu")
colnames(samples_mu$mu) <- paste0("mu_", 1:L)
colMeans(samples_mu$mu)
mu_alex
```

```{r}
samples_alpha <- extract(knocker::hmc_fit_alex_with_hidden_confounder, "alpha")
dim(samples_alpha$alpha)
samples_alpha_matrix <- matrix(c(colMeans(samples_alpha$alpha[,,1]),
                                 colMeans(samples_alpha$alpha[,,2]),
                                 colMeans(samples_alpha$alpha[,,3]),
                                 colMeans(samples_alpha$alpha[,,4]),
                                 colMeans(samples_alpha$alpha[,,5]),
                                 colMeans(samples_alpha$alpha[,,6]),
                                 colMeans(samples_alpha$alpha[,,7]),
                                 colMeans(samples_alpha$alpha[,,8])),
                               byrow = FALSE, nrow = 5, ncol = 8, dimnames = list(paste0("alpha_", 1:L), paste0("alpha_", 1:N)))
samples_alpha_matrix
alpha_alex
```

```{r}
samples_gamma <- extract(knocker::hmc_fit_alex_with_hidden_confounder, "gamma")
colMeans(samples_gamma$gamma[,,1])
gamma_alex
```

It doesn't learn the parameters correctly except for the first 8 parameters of $\gamma$, which are essentially $\beta$ parameters!
